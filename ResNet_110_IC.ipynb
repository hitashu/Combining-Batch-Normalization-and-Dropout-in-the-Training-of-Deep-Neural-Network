{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet_110_IC.ipynb","provenance":[{"file_id":"1N9WJdrnd9k1AxLePbTeoJxellLYOB3ep","timestamp":1618703001704}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPrXQzfRFGQ/EzBlkgpotA3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JOh1EBxsoi_l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618786480995,"user_tz":240,"elapsed":775,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"a2e4fa01-bd55-4111-ca8a-be62ac55c89f"},"source":["import torch\n","print(torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.8.1+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kaC0bhEIpzCA"},"source":["import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torch import nn, optim\n","import matplotlib.pyplot as plt\n","from time import time\n","import numpy as np\n","import pandas as pd\n","from six.moves import urllib\n","import random\n","from skimage.util import random_noise\n","from math import log10\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6KybXihp0uz","executionInfo":{"status":"ok","timestamp":1618786483137,"user_tz":240,"elapsed":708,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"7c6e5b41-c289-43a4-f257-ab400d16b19a"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-Kx9c67p2rS","executionInfo":{"status":"ok","timestamp":1618786486830,"user_tz":240,"elapsed":856,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"bb40a55b-4b24-48e2-9dd4-3f869bb998d4"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Apr 18 22:54:46 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    25W / 300W |      2MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uq3HH_Bhp4K3","executionInfo":{"status":"ok","timestamp":1618786492380,"user_tz":240,"elapsed":2242,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"8312b183-d14c-4537-fd29-d8b25bc2dbd0"},"source":["# Read the train and test sets of CIFAR-10 data\n","default_transform = transforms.Compose([transforms.ToTensor()])\n","cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=default_transform)\n","cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=default_transform)\n","print(\"Training set size:\", len(cifar_trainset))\n","print(\"Test set size:\", len(cifar_testset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Training set size: 50000\n","Test set size: 10000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DF4zplRYp6fo"},"source":["# Initialize data loader functions\n","BATCH_SIZE = 64\n","train_dataLoader = DataLoader(cifar_trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataLoader = DataLoader(cifar_testset, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rQiG0Pvp-wG","executionInfo":{"status":"ok","timestamp":1618786500952,"user_tz":240,"elapsed":1233,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"70ac367d-21f3-4e23-ecee-7643f27e783f"},"source":["# Validate shape of the input images\n","dataiter = iter(train_dataLoader)\n","images, labels = dataiter.next()\n","\n","print(images.shape)\n","print(labels.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([64, 3, 32, 32])\n","torch.Size([64])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8sbZaovyuatx"},"source":["def conv1x1(in_channels, out_channels, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynsds1GMp-tq"},"source":["def conv3x3(in_channels, out_channels, stride=1):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n","                     stride=stride, padding=1, bias=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cxro3xmp-n_"},"source":["class ResidualBlock(nn.Module):\n","    expansion: int = 1\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPMec9HLFXED"},"source":["class Bottleneck(nn.Module):\n","    expansion = 4  # # output cahnnels / # input channels\n","\n","    def __init__(self, inplanes, outplanes, stride=1):\n","        assert outplanes % self.expansion == 0\n","        super(Bottleneck, self).__init__()\n","        self.inplanes = inplanes\n","        self.outplanes = outplanes\n","        self.bottleneck_planes = int(outplanes / self.expansion)\n","        self.stride = stride\n","\n","        self._make_layer()\n","\n","    def _make_layer(self):\n","        # conv 1x1\n","        self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        self.conv1 = nn.Conv2d(self.inplanes, self.bottleneck_planes,\n","                               kernel_size=1, stride=self.stride, bias=False)\n","        # conv 3x3\n","        self.bn2 = nn.BatchNorm2d(self.bottleneck_planes)\n","        self.conv2 = nn.Conv2d(self.bottleneck_planes, self.bottleneck_planes,\n","                               kernel_size=3, stride=1, padding=1, bias=False)\n","        # conv 1x1\n","        self.bn3 = nn.BatchNorm2d(self.bottleneck_planes)\n","        self.conv3 = nn.Conv2d(self.bottleneck_planes, self.outplanes, kernel_size=1,\n","                               stride=1)\n","        if self.inplanes != self.outplanes:\n","            self.shortcut = nn.Conv2d(self.inplanes, self.outplanes, kernel_size=1,\n","                                      stride=self.stride, bias=False)\n","        else:\n","            self.shortcut = None\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        residual = x\n","        # we do pre-activation\n","        out = self.relu(self.bn1(x))\n","        out = self.conv1(out)\n","\n","        out = self.relu(self.bn2(out))\n","        out = self.conv2(out)\n","\n","        out = self.relu(self.bn3(out))\n","        out = self.conv3(out)\n","\n","        if self.shortcut is not None:\n","            residual = self.shortcut(residual)\n","\n","        out += residual\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZ4bhu90p-gY"},"source":["# ResNet (Referenced from Pytorch official tutorial)\n","class ResNet(nn.Module):\n","    def __init__(self, block, layers, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_channels = 16\n","        # self.conv = conv3x3(3, in_channels)\n","        self.conv = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn = nn.BatchNorm2d(self.in_channels)\n","        # self.relu = nn.ReLU(inplace=True)\n","        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self.make_layer(block, 16, layers[0], stride=1)\n","        self.layer2 = self.make_layer(block, 32, layers[1], stride=2)\n","        self.layer3 = self.make_layer(block, 64, layers[2], stride=2)\n","        # self.layer4 = self.make_layer(block, 512, layers[3], stride=2)\n","\n","        \n","        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(64, num_classes)\n","        # self.conv_out = conv3x3(64,3)\n","\n","    def make_layer(self, block, out_channels, blocks, stride=1):\n","        downsample = None\n","        if ((stride != 1) or (self.in_channels != out_channels * block.expansion)):\n","            downsample = nn.Sequential(\n","                conv1x1(self.in_channels, out_channels*block.expansion, stride=stride),\n","                nn.BatchNorm2d(out_channels*block.expansion))\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride, downsample))\n","        self.in_channels = out_channels * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.in_channels, out_channels))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.bn(out)\n","        out = F.relu(out)\n","        # out=self.maxpool(out)\n","\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        # out = self.layer4(out)\n","\n","        out = F.avg_pool2d(out, out.size()[3])\n","        out = out.view(out.size(0), -1)\n","        # out = torch.flatten(out, 1)\n","        out = self.fc(out)\n","\n","        # out = self.conv_out(out)\n","        \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgGZ8GyTp-SR"},"source":["def get_accuracy(model, dataloader):\n","  \"\"\"\n","    Function to compute accuracy given a model (of class nn) and a dataloader object\n","  \"\"\"\n","  \n","  model.eval()\n","  correct_predictions = 0\n","  with torch.no_grad():\n","    for images, labels in dataloader:\n","      imgs = images.to(device)\n","      lbls = labels.to(device)\n","      # images = images.view(images.shape[0], -1)\n","      output = model(imgs)\n","      _, predicted = torch.max(output.data, 1)\n","      correct_predictions += (predicted == lbls).sum().item()\n","  accuracy = (correct_predictions / len(dataloader.dataset)) * 100\n","  return(accuracy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Igyk4NWTd5pq"},"source":["def train_network(model, num_epochs, learning_rate, train_dataLoader, test_dataLoader, lr_update_rule):\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  train_accuracy_list, test_accuracy_list, train_loss_list = [], [], [] \n","  for epoch in range(num_epochs):\n","    running_loss = 0\n","    for images, labels in train_dataLoader:\n","      imgs = images.to(device)\n","      lbls = labels.to(device)\n","  \n","      # Training step\n","      optimizer.zero_grad()\n","      out = model(imgs)\n","      loss = criterion(out, lbls)\n","      \n","      # Backpropagate loss\n","      loss.backward()\n","      \n","      # Optimize weights\n","      optimizer.step()\n","      \n","      running_loss += loss.item()\n","    \n","    train_loss = running_loss/len(train_dataLoader)\n","    train_loss_list.append(train_loss)\n","    train_accuracy = get_accuracy(model, train_dataLoader)\n","    train_accuracy_list.append(train_accuracy)\n","    \n","    test_accuracy = get_accuracy(model, test_dataLoader)\n","    test_accuracy_list.append(test_accuracy)\n","    print(\"Epoch: {} \\t Training loss: {} \\t Training accuracy: {} \\t Test accuracy: {}\".format(epoch, train_loss, train_accuracy, test_accuracy))\n","    \n","  return model, train_accuracy_list, test_accuracy_list, train_loss_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0O6y3f7qIWt"},"source":["# Hyper-parameters\n","EPOCHS = 100\n","lr = 0.001\n","lr_update = {80:0.0001, 120:0.00001, 160:0.000001}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eLmrg5rqKJe"},"source":["# RESNET-110\n","resnet110 = ResNet(ResidualBlock, [18,18,18]).to(device)\n","\n","# RESNET-110B\n","resnet110B = ResNet(Bottleneck, [18,18,18]).to(device)\n","\n","# RESNET-164\n","resnet164 = ResNet(ResidualBlock, 164, 10).to(device)\n","\n","# RESNET-164B\n","resnet164B = ResNet(Bottleneck, 164, 10).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wi2dooAqLn0","executionInfo":{"status":"ok","timestamp":1618799086884,"user_tz":240,"elapsed":12508576,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"8939af7b-fa95-4cd2-9a20-c06e0efe159e"},"source":["resnet110, train_acc_110, test_acc_110, train_loss_110 = train_network(model=resnet110,\n","                                                                       num_epochs=EPOCHS,\n","                                                                       learning_rate=lr,\n","                                                                       train_dataLoader=train_dataLoader,\n","                                                                       test_dataLoader=test_dataLoader,\n","                                                                       lr_update_rule=lr_update)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 \t Training loss: 1.58185654253606 \t Training accuracy: 54.164 \t Test accuracy: 53.92\n","Epoch: 1 \t Training loss: 2.157213936224008 \t Training accuracy: 27.88 \t Test accuracy: 28.23\n","Epoch: 2 \t Training loss: 1.7455091256924602 \t Training accuracy: 36.356 \t Test accuracy: 36.35\n","Epoch: 3 \t Training loss: 1.5720981435702586 \t Training accuracy: 44.73 \t Test accuracy: 45.050000000000004\n","Epoch: 4 \t Training loss: 1.4512611833374824 \t Training accuracy: 50.014 \t Test accuracy: 49.86\n","Epoch: 5 \t Training loss: 1.3342212248793648 \t Training accuracy: 55.152 \t Test accuracy: 53.76\n","Epoch: 6 \t Training loss: 1.2346964525749615 \t Training accuracy: 59.666 \t Test accuracy: 57.63\n","Epoch: 7 \t Training loss: 1.13784738048873 \t Training accuracy: 63.522 \t Test accuracy: 60.01\n","Epoch: 8 \t Training loss: 1.0481400229132083 \t Training accuracy: 64.544 \t Test accuracy: 60.34\n","Epoch: 9 \t Training loss: 0.9694187131226825 \t Training accuracy: 66.608 \t Test accuracy: 61.42999999999999\n","Epoch: 10 \t Training loss: 0.891709826105391 \t Training accuracy: 72.15599999999999 \t Test accuracy: 64.25\n","Epoch: 11 \t Training loss: 0.8196933212716256 \t Training accuracy: 71.292 \t Test accuracy: 62.82\n","Epoch: 12 \t Training loss: 0.7446910901676358 \t Training accuracy: 76.22 \t Test accuracy: 64.88000000000001\n","Epoch: 13 \t Training loss: 0.675898624579315 \t Training accuracy: 79.938 \t Test accuracy: 64.89\n","Epoch: 14 \t Training loss: 0.5942646989127254 \t Training accuracy: 78.268 \t Test accuracy: 63.89\n","Epoch: 15 \t Training loss: 0.5206788536113547 \t Training accuracy: 83.81 \t Test accuracy: 65.63\n","Epoch: 16 \t Training loss: 0.450215234152039 \t Training accuracy: 86.012 \t Test accuracy: 65.19\n","Epoch: 17 \t Training loss: 0.39317206643960056 \t Training accuracy: 88.042 \t Test accuracy: 64.42999999999999\n","Epoch: 18 \t Training loss: 0.32897725553654344 \t Training accuracy: 92.91199999999999 \t Test accuracy: 66.16\n","Epoch: 19 \t Training loss: 0.2922572066049899 \t Training accuracy: 89.742 \t Test accuracy: 65.23\n","Epoch: 20 \t Training loss: 0.25238924439224747 \t Training accuracy: 93.28800000000001 \t Test accuracy: 65.16\n","Epoch: 21 \t Training loss: 0.22549342413139922 \t Training accuracy: 95.61 \t Test accuracy: 66.13\n","Epoch: 22 \t Training loss: 0.1967837104521444 \t Training accuracy: 94.312 \t Test accuracy: 64.69\n","Epoch: 23 \t Training loss: 0.18838886238272537 \t Training accuracy: 93.338 \t Test accuracy: 64.7\n","Epoch: 24 \t Training loss: 0.17506661818927283 \t Training accuracy: 93.54 \t Test accuracy: 64.82\n","Epoch: 25 \t Training loss: 0.1660568295118144 \t Training accuracy: 97.236 \t Test accuracy: 65.31\n","Epoch: 26 \t Training loss: 0.15513586795048032 \t Training accuracy: 96.758 \t Test accuracy: 65.36\n","Epoch: 27 \t Training loss: 0.14765276229652144 \t Training accuracy: 93.194 \t Test accuracy: 64.64\n","Epoch: 28 \t Training loss: 0.1441877009704366 \t Training accuracy: 97.836 \t Test accuracy: 65.42\n","Epoch: 29 \t Training loss: 0.12989090214652554 \t Training accuracy: 96.214 \t Test accuracy: 64.41\n","Epoch: 30 \t Training loss: 0.12798757755609652 \t Training accuracy: 95.872 \t Test accuracy: 64.22\n","Epoch: 31 \t Training loss: 0.11403077559264573 \t Training accuracy: 96.61999999999999 \t Test accuracy: 64.95\n","Epoch: 32 \t Training loss: 0.12055046931909555 \t Training accuracy: 96.242 \t Test accuracy: 64.71000000000001\n","Epoch: 33 \t Training loss: 0.1196946501815239 \t Training accuracy: 97.378 \t Test accuracy: 66.24\n","Epoch: 34 \t Training loss: 0.11248087450323621 \t Training accuracy: 97.69200000000001 \t Test accuracy: 65.97\n","Epoch: 35 \t Training loss: 0.10680048809984646 \t Training accuracy: 97.642 \t Test accuracy: 65.93\n","Epoch: 36 \t Training loss: 0.0923872730769741 \t Training accuracy: 95.234 \t Test accuracy: 63.75999999999999\n","Epoch: 37 \t Training loss: 0.10803112183528407 \t Training accuracy: 96.87 \t Test accuracy: 64.86\n","Epoch: 38 \t Training loss: 0.09286542566817092 \t Training accuracy: 97.19200000000001 \t Test accuracy: 65.44\n","Epoch: 39 \t Training loss: 0.09632643736074761 \t Training accuracy: 98.274 \t Test accuracy: 65.74\n","Epoch: 40 \t Training loss: 0.09752412598110888 \t Training accuracy: 97.914 \t Test accuracy: 66.09\n","Epoch: 41 \t Training loss: 0.09685398346048964 \t Training accuracy: 97.652 \t Test accuracy: 65.45\n","Epoch: 42 \t Training loss: 0.08422641958588796 \t Training accuracy: 97.88799999999999 \t Test accuracy: 65.42999999999999\n","Epoch: 43 \t Training loss: 0.08708821055526986 \t Training accuracy: 93.556 \t Test accuracy: 62.74999999999999\n","Epoch: 44 \t Training loss: 0.08890111309230861 \t Training accuracy: 97.81400000000001 \t Test accuracy: 65.64999999999999\n","Epoch: 45 \t Training loss: 0.0768003940190453 \t Training accuracy: 97.08 \t Test accuracy: 64.62\n","Epoch: 46 \t Training loss: 0.08316339399276154 \t Training accuracy: 97.58200000000001 \t Test accuracy: 65.5\n","Epoch: 47 \t Training loss: 0.07758652061566977 \t Training accuracy: 98.276 \t Test accuracy: 65.42\n","Epoch: 48 \t Training loss: 0.07548393150005499 \t Training accuracy: 98.19 \t Test accuracy: 65.11\n","Epoch: 49 \t Training loss: 0.07647343024389956 \t Training accuracy: 95.96000000000001 \t Test accuracy: 63.690000000000005\n","Epoch: 50 \t Training loss: 0.07044677313624421 \t Training accuracy: 96.95 \t Test accuracy: 64.2\n","Epoch: 51 \t Training loss: 0.07920737031554265 \t Training accuracy: 98.298 \t Test accuracy: 65.59\n","Epoch: 52 \t Training loss: 0.07205932209496875 \t Training accuracy: 98.58200000000001 \t Test accuracy: 66.32000000000001\n","Epoch: 53 \t Training loss: 0.07307060704122315 \t Training accuracy: 97.53 \t Test accuracy: 64.33\n","Epoch: 54 \t Training loss: 0.060442688369044505 \t Training accuracy: 97.528 \t Test accuracy: 66.2\n","Epoch: 55 \t Training loss: 0.07110070303091993 \t Training accuracy: 97.68599999999999 \t Test accuracy: 65.42999999999999\n","Epoch: 56 \t Training loss: 0.06788498531301122 \t Training accuracy: 97.30799999999999 \t Test accuracy: 64.39\n","Epoch: 57 \t Training loss: 0.06851864183235847 \t Training accuracy: 98.392 \t Test accuracy: 65.7\n","Epoch: 58 \t Training loss: 0.061441969312846546 \t Training accuracy: 98.7 \t Test accuracy: 65.52\n","Epoch: 59 \t Training loss: 0.05893402281122001 \t Training accuracy: 98.682 \t Test accuracy: 64.9\n","Epoch: 60 \t Training loss: 0.06532112789197998 \t Training accuracy: 97.726 \t Test accuracy: 65.28\n","Epoch: 61 \t Training loss: 0.06021335670817405 \t Training accuracy: 98.694 \t Test accuracy: 66.08000000000001\n","Epoch: 62 \t Training loss: 0.06157661730493836 \t Training accuracy: 98.504 \t Test accuracy: 65.67\n","Epoch: 63 \t Training loss: 0.055250604486609796 \t Training accuracy: 98.464 \t Test accuracy: 65.63\n","Epoch: 64 \t Training loss: 0.06308477779682439 \t Training accuracy: 97.8 \t Test accuracy: 65.14\n","Epoch: 65 \t Training loss: 0.05275966352134076 \t Training accuracy: 97.962 \t Test accuracy: 64.22\n","Epoch: 66 \t Training loss: 0.06133525020173748 \t Training accuracy: 98.508 \t Test accuracy: 66.05\n","Epoch: 67 \t Training loss: 0.05556175114845683 \t Training accuracy: 99.104 \t Test accuracy: 66.03999999999999\n","Epoch: 68 \t Training loss: 0.05035067745156901 \t Training accuracy: 98.912 \t Test accuracy: 65.25999999999999\n","Epoch: 69 \t Training loss: 0.05873619942539407 \t Training accuracy: 96.504 \t Test accuracy: 64.55\n","Epoch: 70 \t Training loss: 0.046586486031203064 \t Training accuracy: 97.83800000000001 \t Test accuracy: 65.21000000000001\n","Epoch: 71 \t Training loss: 0.04843748620424844 \t Training accuracy: 98.81400000000001 \t Test accuracy: 65.33\n","Epoch: 72 \t Training loss: 0.049723210960270506 \t Training accuracy: 98.848 \t Test accuracy: 66.02\n","Epoch: 73 \t Training loss: 0.05373930083209639 \t Training accuracy: 98.0 \t Test accuracy: 65.11\n","Epoch: 74 \t Training loss: 0.05702598980697029 \t Training accuracy: 98.18599999999999 \t Test accuracy: 64.77000000000001\n","Epoch: 75 \t Training loss: 0.04726564088452827 \t Training accuracy: 97.866 \t Test accuracy: 65.22\n","Epoch: 76 \t Training loss: 0.048463681315713804 \t Training accuracy: 98.732 \t Test accuracy: 65.85\n","Epoch: 77 \t Training loss: 0.04522120274241318 \t Training accuracy: 98.468 \t Test accuracy: 65.77\n","Epoch: 78 \t Training loss: 0.04797302835095135 \t Training accuracy: 98.344 \t Test accuracy: 64.94\n","Epoch: 79 \t Training loss: 0.046431528339448296 \t Training accuracy: 98.554 \t Test accuracy: 64.81\n","Epoch: 80 \t Training loss: 0.047711718456058635 \t Training accuracy: 98.454 \t Test accuracy: 65.64\n","Epoch: 81 \t Training loss: 0.04563353667014857 \t Training accuracy: 99.152 \t Test accuracy: 65.19\n","Epoch: 82 \t Training loss: 0.044266514889577335 \t Training accuracy: 99.3 \t Test accuracy: 65.9\n","Epoch: 83 \t Training loss: 0.04467824336712706 \t Training accuracy: 98.142 \t Test accuracy: 65.3\n","Epoch: 84 \t Training loss: 0.0389878554062744 \t Training accuracy: 99.10600000000001 \t Test accuracy: 65.47\n","Epoch: 85 \t Training loss: 0.047698291421220625 \t Training accuracy: 99.102 \t Test accuracy: 66.14999999999999\n","Epoch: 86 \t Training loss: 0.04116311950979473 \t Training accuracy: 98.29599999999999 \t Test accuracy: 65.05\n","Epoch: 87 \t Training loss: 0.040937421314479056 \t Training accuracy: 99.016 \t Test accuracy: 65.77\n","Epoch: 88 \t Training loss: 0.04748522429941389 \t Training accuracy: 99.41 \t Test accuracy: 66.22\n","Epoch: 89 \t Training loss: 0.04074151884562567 \t Training accuracy: 99.28200000000001 \t Test accuracy: 65.77\n","Epoch: 90 \t Training loss: 0.04167489935306039 \t Training accuracy: 99.22800000000001 \t Test accuracy: 65.59\n","Epoch: 91 \t Training loss: 0.03778390319283416 \t Training accuracy: 98.982 \t Test accuracy: 65.83\n","Epoch: 92 \t Training loss: 0.04314462726047296 \t Training accuracy: 99.422 \t Test accuracy: 66.38\n","Epoch: 93 \t Training loss: 0.03904479772586863 \t Training accuracy: 99.196 \t Test accuracy: 65.39\n","Epoch: 94 \t Training loss: 0.03740971547636606 \t Training accuracy: 98.708 \t Test accuracy: 65.91\n","Epoch: 95 \t Training loss: 0.046882972563676475 \t Training accuracy: 98.94800000000001 \t Test accuracy: 65.83\n","Epoch: 96 \t Training loss: 0.030362031640666698 \t Training accuracy: 98.976 \t Test accuracy: 65.98\n","Epoch: 97 \t Training loss: 0.040998241669089594 \t Training accuracy: 97.91 \t Test accuracy: 65.07\n","Epoch: 98 \t Training loss: 0.03704176832016086 \t Training accuracy: 99.304 \t Test accuracy: 66.82000000000001\n","Epoch: 99 \t Training loss: 0.03465426920736304 \t Training accuracy: 99.222 \t Test accuracy: 66.5\n","Epoch: 100 \t Training loss: 0.03499339159396168 \t Training accuracy: 98.58200000000001 \t Test accuracy: 65.01\n","Epoch: 101 \t Training loss: 0.03471934811941281 \t Training accuracy: 98.87 \t Test accuracy: 65.58\n","Epoch: 102 \t Training loss: 0.03575611167097264 \t Training accuracy: 98.002 \t Test accuracy: 65.41\n","Epoch: 103 \t Training loss: 0.039884854003961516 \t Training accuracy: 99.02 \t Test accuracy: 65.58\n","Epoch: 104 \t Training loss: 0.037643938160399 \t Training accuracy: 98.86399999999999 \t Test accuracy: 66.11\n","Epoch: 105 \t Training loss: 0.0331892106148219 \t Training accuracy: 99.384 \t Test accuracy: 65.91\n","Epoch: 106 \t Training loss: 0.034111441487533964 \t Training accuracy: 99.238 \t Test accuracy: 66.13\n","Epoch: 107 \t Training loss: 0.03147628951570331 \t Training accuracy: 98.976 \t Test accuracy: 65.52\n","Epoch: 108 \t Training loss: 0.0314709678079355 \t Training accuracy: 98.54 \t Test accuracy: 64.75999999999999\n","Epoch: 109 \t Training loss: 0.03239797249607875 \t Training accuracy: 98.588 \t Test accuracy: 65.25999999999999\n","Epoch: 110 \t Training loss: 0.037349385882938006 \t Training accuracy: 99.49 \t Test accuracy: 67.15\n","Epoch: 111 \t Training loss: 0.03134841640898779 \t Training accuracy: 98.69 \t Test accuracy: 65.60000000000001\n","Epoch: 112 \t Training loss: 0.03589981226400887 \t Training accuracy: 98.786 \t Test accuracy: 64.95\n","Epoch: 113 \t Training loss: 0.02736158557804873 \t Training accuracy: 99.592 \t Test accuracy: 66.51\n","Epoch: 114 \t Training loss: 0.03919556165988128 \t Training accuracy: 99.39 \t Test accuracy: 65.86\n","Epoch: 115 \t Training loss: 0.025074376003800945 \t Training accuracy: 98.94800000000001 \t Test accuracy: 65.49000000000001\n","Epoch: 116 \t Training loss: 0.030104955581984186 \t Training accuracy: 99.28200000000001 \t Test accuracy: 66.22\n","Epoch: 117 \t Training loss: 0.03157012300387047 \t Training accuracy: 99.206 \t Test accuracy: 65.38000000000001\n","Epoch: 118 \t Training loss: 0.032583683416305576 \t Training accuracy: 99.304 \t Test accuracy: 65.94\n","Epoch: 119 \t Training loss: 0.03406849832468591 \t Training accuracy: 99.25 \t Test accuracy: 66.31\n","Epoch: 120 \t Training loss: 0.02689114111454842 \t Training accuracy: 99.482 \t Test accuracy: 65.95\n","Epoch: 121 \t Training loss: 0.032496239472863314 \t Training accuracy: 98.532 \t Test accuracy: 65.63\n","Epoch: 122 \t Training loss: 0.03157678592487571 \t Training accuracy: 99.232 \t Test accuracy: 65.16999999999999\n","Epoch: 123 \t Training loss: 0.025546256858652132 \t Training accuracy: 99.278 \t Test accuracy: 65.14999999999999\n","Epoch: 124 \t Training loss: 0.02884588475896107 \t Training accuracy: 98.83800000000001 \t Test accuracy: 65.06\n","Epoch: 125 \t Training loss: 0.030507988981747627 \t Training accuracy: 98.912 \t Test accuracy: 65.3\n","Epoch: 126 \t Training loss: 0.029930885467795765 \t Training accuracy: 99.21799999999999 \t Test accuracy: 66.13\n","Epoch: 127 \t Training loss: 0.03380748006055796 \t Training accuracy: 98.79400000000001 \t Test accuracy: 64.97\n","Epoch: 128 \t Training loss: 0.025564545804559958 \t Training accuracy: 99.376 \t Test accuracy: 65.33\n","Epoch: 129 \t Training loss: 0.032991666644730494 \t Training accuracy: 99.138 \t Test accuracy: 66.36999999999999\n","Epoch: 130 \t Training loss: 0.023621409725694223 \t Training accuracy: 98.672 \t Test accuracy: 64.57000000000001\n","Epoch: 131 \t Training loss: 0.030461593892609076 \t Training accuracy: 99.03 \t Test accuracy: 66.39\n","Epoch: 132 \t Training loss: 0.02677772596431271 \t Training accuracy: 99.414 \t Test accuracy: 66.61\n","Epoch: 133 \t Training loss: 0.026074840698994566 \t Training accuracy: 99.152 \t Test accuracy: 65.49000000000001\n","Epoch: 134 \t Training loss: 0.02532537293724169 \t Training accuracy: 99.488 \t Test accuracy: 65.69\n","Epoch: 135 \t Training loss: 0.028864981166732882 \t Training accuracy: 98.898 \t Test accuracy: 65.94\n","Epoch: 136 \t Training loss: 0.030250582632215996 \t Training accuracy: 99.636 \t Test accuracy: 66.31\n","Epoch: 137 \t Training loss: 0.024980187367954855 \t Training accuracy: 99.542 \t Test accuracy: 66.22\n","Epoch: 138 \t Training loss: 0.027774456432489665 \t Training accuracy: 99.568 \t Test accuracy: 66.5\n","Epoch: 139 \t Training loss: 0.025599395166831705 \t Training accuracy: 99.554 \t Test accuracy: 66.38\n","Epoch: 140 \t Training loss: 0.02467996037330406 \t Training accuracy: 99.516 \t Test accuracy: 65.91\n","Epoch: 141 \t Training loss: 0.026232623054435755 \t Training accuracy: 99.368 \t Test accuracy: 65.75999999999999\n","Epoch: 142 \t Training loss: 0.026389041296372404 \t Training accuracy: 98.80799999999999 \t Test accuracy: 65.44\n","Epoch: 143 \t Training loss: 0.027969086615559573 \t Training accuracy: 98.81 \t Test accuracy: 64.56\n","Epoch: 144 \t Training loss: 0.023546460889699056 \t Training accuracy: 99.21600000000001 \t Test accuracy: 65.86\n","Epoch: 145 \t Training loss: 0.026251740175295774 \t Training accuracy: 99.47200000000001 \t Test accuracy: 66.3\n","Epoch: 146 \t Training loss: 0.02627952335859545 \t Training accuracy: 99.30799999999999 \t Test accuracy: 65.98\n","Epoch: 147 \t Training loss: 0.03000882817379808 \t Training accuracy: 97.976 \t Test accuracy: 65.13\n","Epoch: 148 \t Training loss: 0.019126892761762618 \t Training accuracy: 99.58200000000001 \t Test accuracy: 65.5\n","Epoch: 149 \t Training loss: 0.022079874655752355 \t Training accuracy: 99.434 \t Test accuracy: 65.97\n","Epoch: 150 \t Training loss: 0.033868478773118 \t Training accuracy: 99.148 \t Test accuracy: 65.46\n","Epoch: 151 \t Training loss: 0.017503435415692877 \t Training accuracy: 99.724 \t Test accuracy: 65.85\n","Epoch: 152 \t Training loss: 0.022514814095328014 \t Training accuracy: 98.912 \t Test accuracy: 64.92\n","Epoch: 153 \t Training loss: 0.02580908253959626 \t Training accuracy: 99.152 \t Test accuracy: 65.13\n","Epoch: 154 \t Training loss: 0.026299356676229034 \t Training accuracy: 99.604 \t Test accuracy: 66.10000000000001\n","Epoch: 155 \t Training loss: 0.021153216133490527 \t Training accuracy: 98.832 \t Test accuracy: 65.21000000000001\n","Epoch: 156 \t Training loss: 0.025524050183657495 \t Training accuracy: 99.298 \t Test accuracy: 65.56\n","Epoch: 157 \t Training loss: 0.019042846419287416 \t Training accuracy: 99.054 \t Test accuracy: 64.87\n","Epoch: 158 \t Training loss: 0.026526808942881914 \t Training accuracy: 99.316 \t Test accuracy: 65.56\n","Epoch: 159 \t Training loss: 0.020469192516008736 \t Training accuracy: 98.932 \t Test accuracy: 65.69\n","Epoch: 160 \t Training loss: 0.02309478192690738 \t Training accuracy: 99.48 \t Test accuracy: 66.52\n","Epoch: 161 \t Training loss: 0.02324157545698912 \t Training accuracy: 99.396 \t Test accuracy: 66.12\n","Epoch: 162 \t Training loss: 0.021590143478031287 \t Training accuracy: 99.464 \t Test accuracy: 66.4\n","Epoch: 163 \t Training loss: 0.02640406503402384 \t Training accuracy: 99.822 \t Test accuracy: 66.38\n","Epoch: 164 \t Training loss: 0.01763927433698754 \t Training accuracy: 99.26 \t Test accuracy: 65.09\n","Epoch: 165 \t Training loss: 0.024845866454153574 \t Training accuracy: 98.664 \t Test accuracy: 64.64\n","Epoch: 166 \t Training loss: 0.02130810831428145 \t Training accuracy: 99.528 \t Test accuracy: 65.56\n","Epoch: 167 \t Training loss: 0.016977517255873886 \t Training accuracy: 99.522 \t Test accuracy: 65.84\n","Epoch: 168 \t Training loss: 0.0265317332197759 \t Training accuracy: 99.55199999999999 \t Test accuracy: 66.06\n","Epoch: 169 \t Training loss: 0.020619676107681558 \t Training accuracy: 99.09 \t Test accuracy: 66.17\n","Epoch: 170 \t Training loss: 0.019554923690879026 \t Training accuracy: 99.628 \t Test accuracy: 65.82000000000001\n","Epoch: 171 \t Training loss: 0.026375487720377726 \t Training accuracy: 99.4 \t Test accuracy: 65.41\n","Epoch: 172 \t Training loss: 0.01845517562715101 \t Training accuracy: 99.766 \t Test accuracy: 66.94\n","Epoch: 173 \t Training loss: 0.022023992533085245 \t Training accuracy: 99.408 \t Test accuracy: 65.9\n","Epoch: 174 \t Training loss: 0.021362551955448592 \t Training accuracy: 99.636 \t Test accuracy: 66.09\n","Epoch: 175 \t Training loss: 0.015926258462803537 \t Training accuracy: 99.56 \t Test accuracy: 66.46\n","Epoch: 176 \t Training loss: 0.023439316533088637 \t Training accuracy: 99.592 \t Test accuracy: 66.14\n","Epoch: 177 \t Training loss: 0.019609358149471314 \t Training accuracy: 98.598 \t Test accuracy: 65.18\n","Epoch: 178 \t Training loss: 0.02668331967697319 \t Training accuracy: 98.94800000000001 \t Test accuracy: 64.88000000000001\n","Epoch: 179 \t Training loss: 0.02067827422625434 \t Training accuracy: 99.78 \t Test accuracy: 66.32000000000001\n","Epoch: 180 \t Training loss: 0.016619542825102817 \t Training accuracy: 98.13 \t Test accuracy: 65.02\n","Epoch: 181 \t Training loss: 0.022999500142489818 \t Training accuracy: 99.328 \t Test accuracy: 65.57\n","Epoch: 182 \t Training loss: 0.016177796953775855 \t Training accuracy: 99.506 \t Test accuracy: 65.49000000000001\n","Epoch: 183 \t Training loss: 0.02067766299137998 \t Training accuracy: 99.55199999999999 \t Test accuracy: 66.11\n","Epoch: 184 \t Training loss: 0.023653468958849413 \t Training accuracy: 99.702 \t Test accuracy: 65.82000000000001\n","Epoch: 185 \t Training loss: 0.019207619081708052 \t Training accuracy: 99.56 \t Test accuracy: 65.44\n","Epoch: 186 \t Training loss: 0.015274717547997991 \t Training accuracy: 99.52 \t Test accuracy: 66.08000000000001\n","Epoch: 187 \t Training loss: 0.018625834718975342 \t Training accuracy: 99.334 \t Test accuracy: 65.72\n","Epoch: 188 \t Training loss: 0.02360818267341696 \t Training accuracy: 99.546 \t Test accuracy: 66.28\n","Epoch: 189 \t Training loss: 0.019464890428683962 \t Training accuracy: 99.67 \t Test accuracy: 65.68\n","Epoch: 190 \t Training loss: 0.014798199462401337 \t Training accuracy: 99.478 \t Test accuracy: 66.28\n","Epoch: 191 \t Training loss: 0.024359382244979674 \t Training accuracy: 99.586 \t Test accuracy: 66.03\n","Epoch: 192 \t Training loss: 0.01498011792412229 \t Training accuracy: 99.212 \t Test accuracy: 66.56\n","Epoch: 193 \t Training loss: 0.02094464709818818 \t Training accuracy: 99.788 \t Test accuracy: 66.11\n","Epoch: 194 \t Training loss: 0.017933886349930286 \t Training accuracy: 99.102 \t Test accuracy: 64.77000000000001\n","Epoch: 195 \t Training loss: 0.020730644236118912 \t Training accuracy: 99.868 \t Test accuracy: 66.55\n","Epoch: 196 \t Training loss: 0.017958259726462537 \t Training accuracy: 99.288 \t Test accuracy: 65.51\n","Epoch: 197 \t Training loss: 0.01621126256591278 \t Training accuracy: 99.48599999999999 \t Test accuracy: 66.01\n","Epoch: 198 \t Training loss: 0.0194431987469276 \t Training accuracy: 99.494 \t Test accuracy: 65.96\n","Epoch: 199 \t Training loss: 0.01956033902425516 \t Training accuracy: 99.646 \t Test accuracy: 66.53999999999999\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SQSxZ1d_N9QH"},"source":["import pandas as pd\n","metric_data_110 = pd.DataFrame({'Epoch': range(1,201), 'Train_Acc': train_acc_110, 'Test_Acc': test_acc_110, 'Train_Loss': train_loss_110})\n","metric_data_110.to_csv('ResNet110.csv', index=False)\n","torch.save(resnet110, 'ResNet110.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DtxunMEGcjg2"},"source":["## IC Layer - Variant 1 (IC --> Conv2D --> ReLU)"]},{"cell_type":"code","metadata":{"id":"HTy1Ht8uqTSC"},"source":["def IC(inputs, p=0.05):\n","  y = nn.Sequential(\n","      nn.BatchNorm2d(inputs),\n","      nn.Dropout(p))\n","\n","  # y = nn.Dropout(p)(y)\n","\n","  return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTgXQCRQdwIY"},"source":["class ResidualBlock_IC_1(nn.Module):\n","    expansion: int = 1\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(ResidualBlock_IC_1, self).__init__()\n","        self.IC1 = IC(in_channels)\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","        # self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu1 = nn.ReLU()\n","        self.IC2 = IC(out_channels)\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","        self.relu2 = nn.ReLU()\n","        # self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.IC1(x)\n","        out = self.conv1(out)\n","        # out = self.bn1(out)\n","        out = self.relu1(out)\n","        out = self.IC2(out)\n","        out = self.conv2(out)\n","        out = self.relu2(out)\n","        # out = self.bn2(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        # out = self.relu(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83Iag00yhqGI"},"source":["# RESNET-110 IC Variant 1\n","resnet110_ic1 = ResNet(ResidualBlock_IC_1, [18,18,18]).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmdfqpQMiKQL","executionInfo":{"status":"ok","timestamp":1618805889135,"user_tz":240,"elapsed":6491270,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"14707e87-45a6-43b8-836f-9851e08b12d5"},"source":["resnet110_ic1, train_acc_110_ic1, test_acc_110_ic1, train_loss_110_ic1 = train_network(model=resnet110_ic1,\n","                                                                       num_epochs=EPOCHS,\n","                                                                       learning_rate=lr,\n","                                                                       train_dataLoader=train_dataLoader,\n","                                                                       test_dataLoader=test_dataLoader,\n","                                                                       lr_update_rule=lr_update)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 \t Training loss: 1.3877640347499067 \t Training accuracy: 66.322 \t Test accuracy: 64.92\n","Epoch: 1 \t Training loss: 0.8935202033547185 \t Training accuracy: 74.68 \t Test accuracy: 71.47\n","Epoch: 2 \t Training loss: 0.6866425842885167 \t Training accuracy: 80.772 \t Test accuracy: 75.8\n","Epoch: 3 \t Training loss: 0.5595522226618074 \t Training accuracy: 85.506 \t Test accuracy: 79.99000000000001\n","Epoch: 4 \t Training loss: 0.4658035437964722 \t Training accuracy: 85.776 \t Test accuracy: 79.33\n","Epoch: 5 \t Training loss: 0.4101467627336455 \t Training accuracy: 88.7 \t Test accuracy: 80.5\n","Epoch: 6 \t Training loss: 0.3418607783439519 \t Training accuracy: 90.468 \t Test accuracy: 80.61\n","Epoch: 7 \t Training loss: 0.29074775788675794 \t Training accuracy: 91.72 \t Test accuracy: 80.65\n","Epoch: 8 \t Training loss: 0.2519302962304991 \t Training accuracy: 93.702 \t Test accuracy: 82.11\n","Epoch: 9 \t Training loss: 0.21307906678513341 \t Training accuracy: 92.85799999999999 \t Test accuracy: 81.3\n","Epoch: 10 \t Training loss: 0.1946131721403345 \t Training accuracy: 92.814 \t Test accuracy: 80.67\n","Epoch: 11 \t Training loss: 0.1749929856613774 \t Training accuracy: 95.214 \t Test accuracy: 82.33\n","Epoch: 12 \t Training loss: 0.15228736987742392 \t Training accuracy: 95.056 \t Test accuracy: 82.17999999999999\n","Epoch: 13 \t Training loss: 0.1467631367318656 \t Training accuracy: 96.488 \t Test accuracy: 82.78999999999999\n","Epoch: 14 \t Training loss: 0.13544150925291432 \t Training accuracy: 97.356 \t Test accuracy: 83.31\n","Epoch: 15 \t Training loss: 0.1169595102716447 \t Training accuracy: 95.528 \t Test accuracy: 81.35\n","Epoch: 16 \t Training loss: 0.11718151802101823 \t Training accuracy: 96.428 \t Test accuracy: 82.04\n","Epoch: 17 \t Training loss: 0.11657308332641106 \t Training accuracy: 98.112 \t Test accuracy: 83.74000000000001\n","Epoch: 18 \t Training loss: 0.10252032595712815 \t Training accuracy: 96.932 \t Test accuracy: 82.35\n","Epoch: 19 \t Training loss: 0.09806199088190085 \t Training accuracy: 97.60799999999999 \t Test accuracy: 82.82000000000001\n","Epoch: 20 \t Training loss: 0.10495657133548271 \t Training accuracy: 98.15 \t Test accuracy: 83.42\n","Epoch: 21 \t Training loss: 0.09502347698464961 \t Training accuracy: 97.638 \t Test accuracy: 83.13000000000001\n","Epoch: 22 \t Training loss: 0.0965283406032559 \t Training accuracy: 97.802 \t Test accuracy: 82.87\n","Epoch: 23 \t Training loss: 0.07757699707209768 \t Training accuracy: 96.87 \t Test accuracy: 82.56\n","Epoch: 24 \t Training loss: 0.09348553715123678 \t Training accuracy: 97.334 \t Test accuracy: 82.07\n","Epoch: 25 \t Training loss: 0.08346498101447111 \t Training accuracy: 97.32 \t Test accuracy: 83.02000000000001\n","Epoch: 26 \t Training loss: 0.08202363535508161 \t Training accuracy: 98.18599999999999 \t Test accuracy: 83.42\n","Epoch: 27 \t Training loss: 0.07472395539894709 \t Training accuracy: 97.718 \t Test accuracy: 83.12\n","Epoch: 28 \t Training loss: 0.0774878031175817 \t Training accuracy: 97.026 \t Test accuracy: 82.23\n","Epoch: 29 \t Training loss: 0.06951851674067952 \t Training accuracy: 98.414 \t Test accuracy: 83.56\n","Epoch: 30 \t Training loss: 0.07789436541497707 \t Training accuracy: 97.69 \t Test accuracy: 82.73\n","Epoch: 31 \t Training loss: 0.0690274969906882 \t Training accuracy: 97.836 \t Test accuracy: 82.39999999999999\n","Epoch: 32 \t Training loss: 0.0642737243387043 \t Training accuracy: 98.53 \t Test accuracy: 83.37\n","Epoch: 33 \t Training loss: 0.06689953420336818 \t Training accuracy: 98.336 \t Test accuracy: 83.24000000000001\n","Epoch: 34 \t Training loss: 0.07059372345848595 \t Training accuracy: 98.366 \t Test accuracy: 83.31\n","Epoch: 35 \t Training loss: 0.06630082023577155 \t Training accuracy: 97.36 \t Test accuracy: 82.61\n","Epoch: 36 \t Training loss: 0.06173251307291953 \t Training accuracy: 98.19 \t Test accuracy: 83.25\n","Epoch: 37 \t Training loss: 0.05888749831414167 \t Training accuracy: 97.966 \t Test accuracy: 82.62\n","Epoch: 38 \t Training loss: 0.0633855075128247 \t Training accuracy: 97.61999999999999 \t Test accuracy: 82.13000000000001\n","Epoch: 39 \t Training loss: 0.06156744835826823 \t Training accuracy: 98.508 \t Test accuracy: 83.41\n","Epoch: 40 \t Training loss: 0.05682571535315061 \t Training accuracy: 97.858 \t Test accuracy: 82.71\n","Epoch: 41 \t Training loss: 0.05489417276946027 \t Training accuracy: 98.624 \t Test accuracy: 84.28\n","Epoch: 42 \t Training loss: 0.05568677635723189 \t Training accuracy: 97.416 \t Test accuracy: 82.94\n","Epoch: 43 \t Training loss: 0.05253879707696659 \t Training accuracy: 98.26 \t Test accuracy: 82.92\n","Epoch: 44 \t Training loss: 0.04844431059568694 \t Training accuracy: 97.124 \t Test accuracy: 82.02000000000001\n","Epoch: 45 \t Training loss: 0.061044634504587914 \t Training accuracy: 98.372 \t Test accuracy: 83.06\n","Epoch: 46 \t Training loss: 0.04875618735756463 \t Training accuracy: 97.98599999999999 \t Test accuracy: 82.52000000000001\n","Epoch: 47 \t Training loss: 0.04850005868500333 \t Training accuracy: 98.258 \t Test accuracy: 83.5\n","Epoch: 48 \t Training loss: 0.052120677041187 \t Training accuracy: 98.622 \t Test accuracy: 82.95\n","Epoch: 49 \t Training loss: 0.0542508537638867 \t Training accuracy: 98.374 \t Test accuracy: 83.23\n","Epoch: 50 \t Training loss: 0.042096192820604755 \t Training accuracy: 98.436 \t Test accuracy: 82.8\n","Epoch: 51 \t Training loss: 0.046962462784153024 \t Training accuracy: 99.144 \t Test accuracy: 84.48\n","Epoch: 52 \t Training loss: 0.04381156358161502 \t Training accuracy: 98.962 \t Test accuracy: 84.11999999999999\n","Epoch: 53 \t Training loss: 0.046586953404294314 \t Training accuracy: 98.588 \t Test accuracy: 83.59\n","Epoch: 54 \t Training loss: 0.05098245541453171 \t Training accuracy: 98.89 \t Test accuracy: 83.91999999999999\n","Epoch: 55 \t Training loss: 0.04729277636822196 \t Training accuracy: 98.788 \t Test accuracy: 83.5\n","Epoch: 56 \t Training loss: 0.04223405288385294 \t Training accuracy: 98.358 \t Test accuracy: 82.22\n","Epoch: 57 \t Training loss: 0.041174147638556 \t Training accuracy: 98.74000000000001 \t Test accuracy: 83.38\n","Epoch: 58 \t Training loss: 0.0424153242299519 \t Training accuracy: 99.056 \t Test accuracy: 83.81\n","Epoch: 59 \t Training loss: 0.034342160864408326 \t Training accuracy: 98.664 \t Test accuracy: 83.5\n","Epoch: 60 \t Training loss: 0.056162549213755936 \t Training accuracy: 98.67399999999999 \t Test accuracy: 82.74000000000001\n","Epoch: 61 \t Training loss: 0.04020236022119476 \t Training accuracy: 99.342 \t Test accuracy: 83.82\n","Epoch: 62 \t Training loss: 0.03924595622526755 \t Training accuracy: 99.10600000000001 \t Test accuracy: 84.14\n","Epoch: 63 \t Training loss: 0.03450804860747206 \t Training accuracy: 99.422 \t Test accuracy: 84.22\n","Epoch: 64 \t Training loss: 0.04830404541046118 \t Training accuracy: 98.60600000000001 \t Test accuracy: 83.22\n","Epoch: 65 \t Training loss: 0.04026666581819398 \t Training accuracy: 98.722 \t Test accuracy: 83.72\n","Epoch: 66 \t Training loss: 0.03673071428813943 \t Training accuracy: 98.59 \t Test accuracy: 83.85000000000001\n","Epoch: 67 \t Training loss: 0.03841072110538258 \t Training accuracy: 98.554 \t Test accuracy: 83.58\n","Epoch: 68 \t Training loss: 0.03687330975274928 \t Training accuracy: 99.10600000000001 \t Test accuracy: 84.22\n","Epoch: 69 \t Training loss: 0.03770754102244884 \t Training accuracy: 98.86399999999999 \t Test accuracy: 83.48\n","Epoch: 70 \t Training loss: 0.03267853576504245 \t Training accuracy: 97.992 \t Test accuracy: 82.39999999999999\n","Epoch: 71 \t Training loss: 0.0339465645372125 \t Training accuracy: 98.91799999999999 \t Test accuracy: 83.35000000000001\n","Epoch: 72 \t Training loss: 0.04222582838725974 \t Training accuracy: 98.92 \t Test accuracy: 83.95\n","Epoch: 73 \t Training loss: 0.0386414135463269 \t Training accuracy: 99.148 \t Test accuracy: 83.99\n","Epoch: 74 \t Training loss: 0.027754144273729815 \t Training accuracy: 99.426 \t Test accuracy: 84.17999999999999\n","Epoch: 75 \t Training loss: 0.03261574298757879 \t Training accuracy: 98.584 \t Test accuracy: 83.64\n","Epoch: 76 \t Training loss: 0.03997540449632382 \t Training accuracy: 98.884 \t Test accuracy: 83.47\n","Epoch: 77 \t Training loss: 0.029557764733494658 \t Training accuracy: 98.992 \t Test accuracy: 83.37\n","Epoch: 78 \t Training loss: 0.038795107555480726 \t Training accuracy: 98.572 \t Test accuracy: 83.72\n","Epoch: 79 \t Training loss: 0.033238948681601495 \t Training accuracy: 98.99 \t Test accuracy: 83.67\n","Epoch: 80 \t Training loss: 0.028714636531607947 \t Training accuracy: 98.988 \t Test accuracy: 83.95\n","Epoch: 81 \t Training loss: 0.03220103851565414 \t Training accuracy: 98.762 \t Test accuracy: 83.15\n","Epoch: 82 \t Training loss: 0.03658708796129105 \t Training accuracy: 99.092 \t Test accuracy: 84.17\n","Epoch: 83 \t Training loss: 0.03204767438562979 \t Training accuracy: 98.626 \t Test accuracy: 82.74000000000001\n","Epoch: 84 \t Training loss: 0.03020043802403552 \t Training accuracy: 99.16 \t Test accuracy: 84.23\n","Epoch: 85 \t Training loss: 0.03166049847377019 \t Training accuracy: 98.654 \t Test accuracy: 83.63000000000001\n","Epoch: 86 \t Training loss: 0.02439578993495938 \t Training accuracy: 99.392 \t Test accuracy: 84.66\n","Epoch: 87 \t Training loss: 0.042962614332291406 \t Training accuracy: 98.59 \t Test accuracy: 83.23\n","Epoch: 88 \t Training loss: 0.02207362940298062 \t Training accuracy: 99.236 \t Test accuracy: 83.99\n","Epoch: 89 \t Training loss: 0.029805368079744085 \t Training accuracy: 99.05000000000001 \t Test accuracy: 83.59\n","Epoch: 90 \t Training loss: 0.028465805158675094 \t Training accuracy: 99.372 \t Test accuracy: 84.02\n","Epoch: 91 \t Training loss: 0.03322870184638537 \t Training accuracy: 99.35199999999999 \t Test accuracy: 84.28999999999999\n","Epoch: 92 \t Training loss: 0.031950234816542106 \t Training accuracy: 99.22 \t Test accuracy: 83.62\n","Epoch: 93 \t Training loss: 0.022172874873684918 \t Training accuracy: 99.068 \t Test accuracy: 83.21\n","Epoch: 94 \t Training loss: 0.030513087559799885 \t Training accuracy: 98.79 \t Test accuracy: 83.26\n","Epoch: 95 \t Training loss: 0.025598931356887154 \t Training accuracy: 98.738 \t Test accuracy: 83.35000000000001\n","Epoch: 96 \t Training loss: 0.02195788125278071 \t Training accuracy: 98.386 \t Test accuracy: 83.00999999999999\n","Epoch: 97 \t Training loss: 0.03479554400917066 \t Training accuracy: 99.092 \t Test accuracy: 84.21\n","Epoch: 98 \t Training loss: 0.02810440640801562 \t Training accuracy: 99.436 \t Test accuracy: 84.13000000000001\n","Epoch: 99 \t Training loss: 0.021963913415568054 \t Training accuracy: 99.49 \t Test accuracy: 84.28999999999999\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2I1O0_TzAa8Z"},"source":["import pandas as pd\n","metric_data_110 = pd.DataFrame({'Epoch': range(1,201), 'Train_Acc': train_acc_110, 'Test_Acc': test_acc_110, 'Train_Loss': train_loss_110})\n","metric_data_110.to_csv('ResNet110.csv', index=False)\n","metric_data_110_ic1 = pd.DataFrame({'Epoch': range(1,EPOCHS+1), 'Train_Acc': train_acc_110_ic1, 'Test_Acc': test_acc_110_ic1, 'Train_Loss': train_loss_110_ic1})\n","metric_data_110_ic1.to_csv('ResNet110_ic1.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMafwspsBHp3"},"source":["torch.save(resnet110, 'ResNet110.pt')\n","torch.save(resnet110_ic1, 'ResNet110_ic1.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMiiwVFJz8zS"},"source":["## Variant 2 (Conv2D --> ReLU --> IC)"]},{"cell_type":"code","metadata":{"id":"vCKv9kcLk70k"},"source":["class ResidualBlock_IC_2(nn.Module):\n","    expansion: int = 1\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(ResidualBlock_IC_2, self).__init__()\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.IC1 = IC(out_channels)\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.IC2 = IC(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.relu1(out)\n","        out = self.IC1(out)\n","        out = self.conv2(out)\n","        out = self.relu2(out)\n","        out = self.IC2(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        # out = self.relu(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5dNQquPJ0NJO"},"source":["resnet110_ic2 = ResNet(ResidualBlock_IC_2, [18,18,18]).to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85Ui3TgyhEKa","executionInfo":{"status":"ok","timestamp":1618812798490,"user_tz":240,"elapsed":6614537,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"3533c6b2-d0ce-4966-ed36-e24f85354821"},"source":["resnet110_ic2, train_acc_110_ic2, test_acc_110_ic2, train_loss_110_ic2 = train_network(model=resnet110_ic2,\n","                                                                       num_epochs=EPOCHS,\n","                                                                       learning_rate=lr,\n","                                                                       train_dataLoader=train_dataLoader,\n","                                                                       test_dataLoader=test_dataLoader,\n","                                                                       lr_update_rule=lr_update)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 \t Training loss: 1.5150970971340414 \t Training accuracy: 56.126 \t Test accuracy: 54.730000000000004\n","Epoch: 1 \t Training loss: 1.1016678426729138 \t Training accuracy: 68.272 \t Test accuracy: 66.47999999999999\n","Epoch: 2 \t Training loss: 0.8361111500150408 \t Training accuracy: 76.538 \t Test accuracy: 72.87\n","Epoch: 3 \t Training loss: 0.6772428837121295 \t Training accuracy: 79.85 \t Test accuracy: 75.63\n","Epoch: 4 \t Training loss: 0.5733010299751521 \t Training accuracy: 83.13199999999999 \t Test accuracy: 77.25\n","Epoch: 5 \t Training loss: 0.4885484798027731 \t Training accuracy: 85.96000000000001 \t Test accuracy: 78.59\n","Epoch: 6 \t Training loss: 0.4242350618971888 \t Training accuracy: 88.8 \t Test accuracy: 80.47\n","Epoch: 7 \t Training loss: 0.36557432578500276 \t Training accuracy: 88.68 \t Test accuracy: 79.83\n","Epoch: 8 \t Training loss: 0.3192051828879377 \t Training accuracy: 91.242 \t Test accuracy: 81.62\n","Epoch: 9 \t Training loss: 0.2727414887265095 \t Training accuracy: 92.84 \t Test accuracy: 81.06\n","Epoch: 10 \t Training loss: 0.24973347618023073 \t Training accuracy: 94.43599999999999 \t Test accuracy: 81.83\n","Epoch: 11 \t Training loss: 0.21616028251645664 \t Training accuracy: 92.768 \t Test accuracy: 80.05\n","Epoch: 12 \t Training loss: 0.17999282980437778 \t Training accuracy: 94.554 \t Test accuracy: 81.10000000000001\n","Epoch: 13 \t Training loss: 0.17674958023964368 \t Training accuracy: 92.51599999999999 \t Test accuracy: 79.3\n","Epoch: 14 \t Training loss: 0.1654058472423688 \t Training accuracy: 94.582 \t Test accuracy: 80.74\n","Epoch: 15 \t Training loss: 0.14759696309652437 \t Training accuracy: 96.286 \t Test accuracy: 81.89\n","Epoch: 16 \t Training loss: 0.13801535794420924 \t Training accuracy: 97.414 \t Test accuracy: 82.96\n","Epoch: 17 \t Training loss: 0.13071155823085961 \t Training accuracy: 97.306 \t Test accuracy: 82.88\n","Epoch: 18 \t Training loss: 0.12268715625678848 \t Training accuracy: 96.00800000000001 \t Test accuracy: 81.63\n","Epoch: 19 \t Training loss: 0.10882723414936982 \t Training accuracy: 97.378 \t Test accuracy: 82.37\n","Epoch: 20 \t Training loss: 0.11227010811567116 \t Training accuracy: 95.488 \t Test accuracy: 81.28\n","Epoch: 21 \t Training loss: 0.11037852785423817 \t Training accuracy: 96.104 \t Test accuracy: 81.44\n","Epoch: 22 \t Training loss: 0.0947493768220915 \t Training accuracy: 96.41 \t Test accuracy: 81.6\n","Epoch: 23 \t Training loss: 0.09740467575114325 \t Training accuracy: 97.31 \t Test accuracy: 82.34\n","Epoch: 24 \t Training loss: 0.08972862676676849 \t Training accuracy: 96.994 \t Test accuracy: 81.55\n","Epoch: 25 \t Training loss: 0.08510675389603105 \t Training accuracy: 98.502 \t Test accuracy: 83.62\n","Epoch: 26 \t Training loss: 0.08132705055957283 \t Training accuracy: 97.898 \t Test accuracy: 82.50999999999999\n","Epoch: 27 \t Training loss: 0.07845793978479046 \t Training accuracy: 97.518 \t Test accuracy: 81.96\n","Epoch: 28 \t Training loss: 0.0795722027283038 \t Training accuracy: 97.648 \t Test accuracy: 82.65\n","Epoch: 29 \t Training loss: 0.073221114701252 \t Training accuracy: 98.14399999999999 \t Test accuracy: 83.22\n","Epoch: 30 \t Training loss: 0.07670099010763695 \t Training accuracy: 98.00999999999999 \t Test accuracy: 82.76\n","Epoch: 31 \t Training loss: 0.06874922867786959 \t Training accuracy: 98.11999999999999 \t Test accuracy: 83.03\n","Epoch: 32 \t Training loss: 0.06735156681549395 \t Training accuracy: 98.036 \t Test accuracy: 82.23\n","Epoch: 33 \t Training loss: 0.06844601545596311 \t Training accuracy: 98.434 \t Test accuracy: 82.93\n","Epoch: 34 \t Training loss: 0.06588868664912856 \t Training accuracy: 98.11999999999999 \t Test accuracy: 82.58\n","Epoch: 35 \t Training loss: 0.06523287642519097 \t Training accuracy: 98.236 \t Test accuracy: 82.49\n","Epoch: 36 \t Training loss: 0.05982870765334374 \t Training accuracy: 98.27799999999999 \t Test accuracy: 82.88\n","Epoch: 37 \t Training loss: 0.05557736990530618 \t Training accuracy: 98.324 \t Test accuracy: 83.02000000000001\n","Epoch: 38 \t Training loss: 0.06300994255559167 \t Training accuracy: 98.708 \t Test accuracy: 82.99\n","Epoch: 39 \t Training loss: 0.051335483669604665 \t Training accuracy: 98.234 \t Test accuracy: 82.80999999999999\n","Epoch: 40 \t Training loss: 0.06035200144995547 \t Training accuracy: 98.496 \t Test accuracy: 82.42\n","Epoch: 41 \t Training loss: 0.04753563202069917 \t Training accuracy: 99.022 \t Test accuracy: 83.87\n","Epoch: 42 \t Training loss: 0.0519049239649778 \t Training accuracy: 97.94 \t Test accuracy: 82.41000000000001\n","Epoch: 43 \t Training loss: 0.05815683522671366 \t Training accuracy: 98.452 \t Test accuracy: 83.0\n","Epoch: 44 \t Training loss: 0.047581314572917366 \t Training accuracy: 98.29 \t Test accuracy: 83.46000000000001\n","Epoch: 45 \t Training loss: 0.04800338495512019 \t Training accuracy: 98.648 \t Test accuracy: 83.32000000000001\n","Epoch: 46 \t Training loss: 0.048247326205576424 \t Training accuracy: 98.98400000000001 \t Test accuracy: 83.48\n","Epoch: 47 \t Training loss: 0.050743877194816475 \t Training accuracy: 98.554 \t Test accuracy: 82.96\n","Epoch: 48 \t Training loss: 0.03623180126082366 \t Training accuracy: 98.578 \t Test accuracy: 82.57\n","Epoch: 49 \t Training loss: 0.05148620073712738 \t Training accuracy: 98.564 \t Test accuracy: 83.14\n","Epoch: 50 \t Training loss: 0.0412201957768657 \t Training accuracy: 98.702 \t Test accuracy: 82.74000000000001\n","Epoch: 51 \t Training loss: 0.040452472666544066 \t Training accuracy: 98.478 \t Test accuracy: 83.04\n","Epoch: 52 \t Training loss: 0.04420624907946397 \t Training accuracy: 97.666 \t Test accuracy: 81.91000000000001\n","Epoch: 53 \t Training loss: 0.047186258239978254 \t Training accuracy: 98.98599999999999 \t Test accuracy: 83.3\n","Epoch: 54 \t Training loss: 0.0361695690985273 \t Training accuracy: 98.706 \t Test accuracy: 82.58\n","Epoch: 55 \t Training loss: 0.034766204858208735 \t Training accuracy: 98.816 \t Test accuracy: 83.38\n","Epoch: 56 \t Training loss: 0.04095784455387885 \t Training accuracy: 98.612 \t Test accuracy: 83.25\n","Epoch: 57 \t Training loss: 0.04245264484452398 \t Training accuracy: 98.85199999999999 \t Test accuracy: 83.22\n","Epoch: 58 \t Training loss: 0.03526487706638182 \t Training accuracy: 98.9 \t Test accuracy: 83.50999999999999\n","Epoch: 59 \t Training loss: 0.033540769007715396 \t Training accuracy: 98.16 \t Test accuracy: 82.55\n","Epoch: 60 \t Training loss: 0.03709777355493839 \t Training accuracy: 98.74000000000001 \t Test accuracy: 83.13000000000001\n","Epoch: 61 \t Training loss: 0.03832596033700141 \t Training accuracy: 99.346 \t Test accuracy: 84.36\n","Epoch: 62 \t Training loss: 0.03605710644770831 \t Training accuracy: 99.414 \t Test accuracy: 83.78\n","Epoch: 63 \t Training loss: 0.02763782557896468 \t Training accuracy: 99.00999999999999 \t Test accuracy: 83.12\n","Epoch: 64 \t Training loss: 0.03723630868940545 \t Training accuracy: 99.506 \t Test accuracy: 84.39999999999999\n","Epoch: 65 \t Training loss: 0.030142896257516125 \t Training accuracy: 99.028 \t Test accuracy: 83.42\n","Epoch: 66 \t Training loss: 0.04147460008781585 \t Training accuracy: 98.34 \t Test accuracy: 83.19\n","Epoch: 67 \t Training loss: 0.033617828752586454 \t Training accuracy: 98.752 \t Test accuracy: 83.23\n","Epoch: 68 \t Training loss: 0.030413004562786908 \t Training accuracy: 97.908 \t Test accuracy: 82.35\n","Epoch: 69 \t Training loss: 0.03365196517066565 \t Training accuracy: 99.32600000000001 \t Test accuracy: 84.11\n","Epoch: 70 \t Training loss: 0.030174524975636743 \t Training accuracy: 99.55199999999999 \t Test accuracy: 83.91999999999999\n","Epoch: 71 \t Training loss: 0.023112563419562485 \t Training accuracy: 99.08200000000001 \t Test accuracy: 83.62\n","Epoch: 72 \t Training loss: 0.03314961984221515 \t Training accuracy: 99.188 \t Test accuracy: 83.87\n","Epoch: 73 \t Training loss: 0.03108447054232074 \t Training accuracy: 99.238 \t Test accuracy: 84.27\n","Epoch: 74 \t Training loss: 0.030183375534180063 \t Training accuracy: 99.19200000000001 \t Test accuracy: 83.75\n","Epoch: 75 \t Training loss: 0.024564644447370027 \t Training accuracy: 99.176 \t Test accuracy: 83.98\n","Epoch: 76 \t Training loss: 0.03262409305663539 \t Training accuracy: 99.386 \t Test accuracy: 84.55\n","Epoch: 77 \t Training loss: 0.02740594088135067 \t Training accuracy: 99.168 \t Test accuracy: 83.26\n","Epoch: 78 \t Training loss: 0.028405715405433813 \t Training accuracy: 99.272 \t Test accuracy: 83.54\n","Epoch: 79 \t Training loss: 0.025605842449806605 \t Training accuracy: 99.264 \t Test accuracy: 84.06\n","Epoch: 80 \t Training loss: 0.030982156183071617 \t Training accuracy: 99.32600000000001 \t Test accuracy: 83.95\n","Epoch: 81 \t Training loss: 0.02836762074149771 \t Training accuracy: 98.97399999999999 \t Test accuracy: 83.24000000000001\n","Epoch: 82 \t Training loss: 0.022017784250921903 \t Training accuracy: 99.658 \t Test accuracy: 84.22\n","Epoch: 83 \t Training loss: 0.023717712804807986 \t Training accuracy: 99.178 \t Test accuracy: 83.57\n","Epoch: 84 \t Training loss: 0.02770225976244621 \t Training accuracy: 98.482 \t Test accuracy: 83.33\n","Epoch: 85 \t Training loss: 0.023022336920596333 \t Training accuracy: 99.426 \t Test accuracy: 84.42\n","Epoch: 86 \t Training loss: 0.025727653622209606 \t Training accuracy: 98.976 \t Test accuracy: 83.72\n","Epoch: 87 \t Training loss: 0.030793854288867457 \t Training accuracy: 99.45 \t Test accuracy: 84.56\n","Epoch: 88 \t Training loss: 0.024311075996727903 \t Training accuracy: 99.682 \t Test accuracy: 84.31\n","Epoch: 89 \t Training loss: 0.022279597302837673 \t Training accuracy: 99.214 \t Test accuracy: 83.64\n","Epoch: 90 \t Training loss: 0.018200549086094774 \t Training accuracy: 99.36 \t Test accuracy: 84.24000000000001\n","Epoch: 91 \t Training loss: 0.02796295446452506 \t Training accuracy: 98.956 \t Test accuracy: 83.39999999999999\n","Epoch: 92 \t Training loss: 0.02672097325952345 \t Training accuracy: 99.28399999999999 \t Test accuracy: 84.02\n","Epoch: 93 \t Training loss: 0.018877361673777297 \t Training accuracy: 99.26 \t Test accuracy: 83.77\n","Epoch: 94 \t Training loss: 0.02530609015114018 \t Training accuracy: 99.58200000000001 \t Test accuracy: 84.97\n","Epoch: 95 \t Training loss: 0.020435985726308593 \t Training accuracy: 99.454 \t Test accuracy: 84.2\n","Epoch: 96 \t Training loss: 0.023311587941806117 \t Training accuracy: 98.884 \t Test accuracy: 82.67\n","Epoch: 97 \t Training loss: 0.022459251359459417 \t Training accuracy: 99.166 \t Test accuracy: 83.61\n","Epoch: 98 \t Training loss: 0.022912497729314397 \t Training accuracy: 98.886 \t Test accuracy: 83.04\n","Epoch: 99 \t Training loss: 0.019161956149230525 \t Training accuracy: 99.522 \t Test accuracy: 84.49\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-MOyOp7FBW2x"},"source":["metric_data_110_ic2 = pd.DataFrame({'Epoch': range(1,EPOCHS+1), 'Train_Acc': train_acc_110_ic2, 'Test_Acc': test_acc_110_ic2, 'Train_Loss': train_loss_110_ic2})\n","metric_data_110_ic2.to_csv('ResNet110_ic2.csv', index=False)\n","torch.save(resnet110_ic2, 'ResNet110_ic2.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HtHDaDGj16pf"},"source":["## Variant 3 (ReLU --> IC --> Conv@D)"]},{"cell_type":"code","metadata":{"id":"7xENG2RI17DA"},"source":["class ResidualBlock_IC_3(nn.Module):\n","    expansion: int = 1\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(ResidualBlock_IC_3, self).__init__()\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.IC1 = IC(in_channels)\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.IC2 = IC(out_channels)\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.relu1(x)\n","        out = self.IC1(out)\n","        out = self.conv1(out)\n","        out = self.relu2(out)\n","        out = self.IC2(out)\n","        out = self.conv2(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        # out = self.relu(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkID80MN17bq"},"source":["resnet110_ic3 = ResNet(ResidualBlock_IC_3, [18,18,18]).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2CQs6yzhdre","executionInfo":{"status":"ok","timestamp":1618780834359,"user_tz":240,"elapsed":652386,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"a92d8893-e20d-42a9-c734-c96a33db9526"},"source":["resnet110_ic3, train_acc_110_ic3, test_acc_110_ic3, train_loss_110_ic3 = train_network(model=resnet110_ic3,\n","                                                                       num_epochs=EPOCHS,\n","                                                                       learning_rate=lr,\n","                                                                       train_dataLoader=train_dataLoader,\n","                                                                       test_dataLoader=test_dataLoader,\n","                                                                       lr_update_rule=lr_update)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 \t Training loss: 1.560406011617397 \t Training accuracy: 53.33 \t Test accuracy: 51.89\n","Epoch: 1 \t Training loss: 1.1350067194617923 \t Training accuracy: 68.826 \t Test accuracy: 66.38\n","Epoch: 2 \t Training loss: 0.8720918902960579 \t Training accuracy: 72.05 \t Test accuracy: 68.41000000000001\n","Epoch: 3 \t Training loss: 0.730270568900706 \t Training accuracy: 76.53999999999999 \t Test accuracy: 71.78999999999999\n","Epoch: 4 \t Training loss: 0.6210320735602732 \t Training accuracy: 82.854 \t Test accuracy: 77.31\n","Epoch: 5 \t Training loss: 0.5423417592521214 \t Training accuracy: 85.15 \t Test accuracy: 78.38000000000001\n","Epoch: 6 \t Training loss: 0.4644252937048902 \t Training accuracy: 88.012 \t Test accuracy: 80.19\n","Epoch: 7 \t Training loss: 0.4105830499735635 \t Training accuracy: 89.024 \t Test accuracy: 79.28\n","Epoch: 8 \t Training loss: 0.3513844155318216 \t Training accuracy: 90.62 \t Test accuracy: 80.54\n","Epoch: 9 \t Training loss: 0.30647588770865175 \t Training accuracy: 90.582 \t Test accuracy: 80.19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"92I1-CNnBlPx"},"source":["metric_data_110_ic3 = pd.DataFrame({'Epoch': range(1,EPOCHS+1), 'Train_Acc': train_acc_110_ic3, 'Test_Acc': test_acc_110_ic3, 'Train_Loss': train_loss_110_ic3})\n","metric_data_110_ic3.to_csv('ResNet110_ic3.csv', index=False)\n","torch.save(resnet110_ic3, 'ResNet110_ic3.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"0Ksek72I18dV","executionInfo":{"status":"ok","timestamp":1618780886302,"user_tz":240,"elapsed":1747,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"e5cd2fee-fb59-43a6-cddc-1202042ead28"},"source":[" # Comparing performance\n","import matplotlib.pyplot as plt\n","filename = 'resnet110_epochs_'+str(EPOCHS)+'_bs_'+str(BATCH_SIZE)+'.png'\n","plt.plot(range(1, EPOCHS+1),test_acc_110_ic1, label=\"IC - Conv2D - ReLU\")\n","plt.plot(range(1, EPOCHS+1),test_acc_110_ic2, label=\"Conv2D - ReLU - IC\")\n","plt.plot(range(1, EPOCHS+1),test_acc_110_ic3, label=\"ReLU - IC - Conv2D\")\n","plt.plot(range(1, EPOCHS+1),test_acc_110, label=\"Baseline\", color=\"red\")\n","plt.legend()\n","plt.savefig(filename, dpi=500)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"CBGnsD-hqN14","executionInfo":{"status":"error","timestamp":1618772029962,"user_tz":240,"elapsed":248729,"user":{"displayName":"Harshita Ved","photoUrl":"","userId":"02203123290902493390"}},"outputId":"4e14339c-7486-4204-ca78-4d50826b3085"},"source":["### Original training loop\n","'''\n","train_accuracy_list, test_accuracy_list, train_loss_list = [], [], [] \n","for epoch in range(num_epochs):\n","    running_loss = 0\n","    for images, labels in train_dataLoader:\n","        # Flatten 28x28x1 image to 784 vector\n","        # images = images.view(images.shape[0], -1)\n","        imgs = images.to(device)\n","        lbls = labels.to(device)\n","    \n","        # Training step\n","        optimizer.zero_grad()\n","        out = resnet110(imgs)\n","        loss = criterion(out, lbls)\n","        \n","        #  Check train correct preditions\n","        # _, train_predicted = torch.max(out.data, 1)\n","        # train_correct += (train_predicted==labels).sum().item()\n","\n","        # Backpropagate loss\n","        loss.backward()\n","        \n","        # Optimize weights\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","    \n","    # print(\"\\nEpoch {} \\t Training Time (in minutes) = {}\".format(epoch, (time()-start)/60))\n","    # print(\"Epoch {} - Training loss: {}\".format(epoch, running_loss/len(train_dataLoader)))\n","    train_loss = running_loss/len(train_dataLoader)\n","    train_loss_list.append(train_loss)\n","    train_accuracy = get_accuracy(resnet110, train_dataLoader)\n","    # print(train_accuracy)\n","    # print(\"Epoch {} - Training accuracy: {}\".format(epoch, train_accuracy))\n","    # print(\"Epoch {} - Training accuracy - Alt: {}\".format(epoch, train_model(network)))\n","    train_accuracy_list.append(train_accuracy)\n","    test_accuracy = get_accuracy(resnet110, test_dataLoader)\n","    # print(\"Epoch {} - Test accuracy: {}\".format(epoch, test_accuracy))\n","    test_accuracy_list.append(test_accuracy)\n","    print(\"Epoch: {} \\t Training loss: {} \\t Training accuracy: {} \\t Test accuracy: {}\".format(epoch, train_loss, train_accuracy, test_accuracy))\n","  '''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 \t Training loss: 2.0450409602021318 \t Training accuracy: 29.14 \t Test accuracy: 29.799999999999997\n","Epoch: 1 \t Training loss: 5.49911764088799 \t Training accuracy: 10.0 \t Test accuracy: 10.0\n","Epoch: 2 \t Training loss: 2.3032108030050917 \t Training accuracy: 10.0 \t Test accuracy: 10.0\n","Epoch: 3 \t Training loss: 2.303347162578417 \t Training accuracy: 10.0 \t Test accuracy: 10.0\n","Epoch: 4 \t Training loss: 2.303161963172581 \t Training accuracy: 10.0 \t Test accuracy: 10.0\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-eea9a456e90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m# print(train_accuracy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# print(\"Epoch {} - Training accuracy: {}\".format(epoch, train_accuracy))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-b5154dde8ffd>\u001b[0m in \u001b[0;36mget_accuracy\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mlbls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m# images = images.view(images.shape[0], -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-cfe320212a32>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# out=self.maxpool(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-ece6f1868cd1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}