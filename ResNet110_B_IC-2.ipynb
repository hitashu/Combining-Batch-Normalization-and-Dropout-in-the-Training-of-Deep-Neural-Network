{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet110_B_IC-2.ipynb","provenance":[{"file_id":"1DaHiCEbysqWuHnMGpLVinJY3FN_aeXPC","timestamp":1618798454779}],"collapsed_sections":[],"authorship_tag":"ABX9TyP23BqNQXbU15nnDsgq0rqx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"FULdXBkv48Jp"},"source":["import torch\n","print(torch.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7tiKdJ85HSg"},"source":["import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torch import nn, optim\n","import matplotlib.pyplot as plt\n","from time import time\n","import numpy as np\n","import pandas as pd\n","from six.moves import urllib\n","import random\n","from skimage.util import random_noise\n","from math import log10\n","import torch.nn.functional as F\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6raDokN95JQc"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCcwo4sU5Lop"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s2bms8mo5Ns1"},"source":["# Read the train and test sets of CIFAR-10 data\n","default_transform = transforms.Compose([transforms.ToTensor()])\n","cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=default_transform)\n","cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=default_transform)\n","print(\"Training set size:\", len(cifar_trainset))\n","print(\"Test set size:\", len(cifar_testset))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zEqaLul5PwC"},"source":["# Initialize data loader functions\n","BATCH_SIZE = 64\n","train_dataLoader = DataLoader(cifar_trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataLoader = DataLoader(cifar_testset, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOP-4pCQ5UTe"},"source":["# Validate shape of the input images\n","dataiter = iter(train_dataLoader)\n","images, labels = dataiter.next()\n","\n","print(images.shape)\n","print(labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0ZcsAZ35VUW"},"source":["def conv1x1(in_channels, out_channels, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bgWC3YO5XhK"},"source":["def conv3x3(in_channels, out_channels, stride=1, groups=1, dilation=1):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n","                     stride=stride, padding=1, groups=groups, bias=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frybWYdkJXcq"},"source":["## Variant 2 (Conv2D --> ReLU --> IC)"]},{"cell_type":"code","metadata":{"id":"gvx0lNerJUtD"},"source":["def IC(inputs, p=0.03):\n","  y = nn.Sequential(\n","      nn.BatchNorm2d(inputs),\n","      nn.Dropout(p))\n","\n","  # y = nn.Dropout(p)(y)\n","\n","  return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LG9FqL935ZoE"},"source":["class Bottleneck_IC2(nn.Module):\n","    expansion = 4  # # output cahnnels / # input channels\n","\n","    def __init__(self, inplanes, outplanes, stride=1):\n","        assert outplanes % self.expansion == 0\n","        super(Bottleneck_IC2, self).__init__()\n","        self.inplanes = inplanes\n","        self.outplanes = outplanes\n","        self.bottleneck_planes = int(outplanes / self.expansion)\n","        self.stride = stride\n","\n","        self._make_layer()\n","\n","    def _make_layer(self):\n","        # conv 1x1\n","        # self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        \n","        self.conv1 = nn.Conv2d(self.inplanes, self.bottleneck_planes,\n","                               kernel_size=1, stride=self.stride, bias=False)\n","        self.IC1 = IC(self.bottleneck_planes)\n","        # conv 3x3\n","        # self.bn2 = nn.BatchNorm2d(self.bottleneck_planes)\n","        \n","        self.conv2 = nn.Conv2d(self.bottleneck_planes, self.bottleneck_planes,\n","                               kernel_size=3, stride=1, padding=1, bias=False)\n","        self.IC2 = IC(self.bottleneck_planes)\n","        # conv 1x1\n","        # self.bn3 = nn.BatchNorm2d(self.bottleneck_planes)\n","        \n","        self.conv3 = nn.Conv2d(self.bottleneck_planes, self.outplanes, kernel_size=1,\n","                               stride=1)\n","        self.IC3 = IC(self.outplanes)\n","\n","        if self.inplanes != self.outplanes:\n","            self.shortcut = nn.Conv2d(self.inplanes, self.outplanes, kernel_size=1,\n","                                      stride=self.stride, bias=False)\n","        else:\n","            self.shortcut = None\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        residual = x\n","  \n","        out = self.conv1(x)\n","        out = self.relu(out)\n","        out = self.IC1(out)\n","\n","        out = self.conv2(out)\n","        out = self.relu(out)\n","        out = self.IC2(out)\n","\n","        out = self.conv3(out)\n","        out = self.relu(out)\n","        out = self.IC3(out)\n","\n","        if self.shortcut is not None:\n","            residual = self.shortcut(residual)\n","\n","        out += residual\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWMw685P5hcL"},"source":["class ResNet(nn.Module):\n","    def __init__(self, block, depth, output_classes=10):\n","        assert (depth - 2) % 9 == 0  # 164 or 1001\n","        super(ResNet_alt, self).__init__()\n","        n = int((depth - 2) / 9)\n","        nstages = [16, 64, 128, 256]\n","        # one conv at the beginning (spatial size: 32x32)\n","        self.conv1 = nn.Conv2d(3, nstages[0], kernel_size=3, stride=1,\n","                               padding=1, bias=False)\n","\n","        # use `block` as unit to construct res-net\n","        # Stage 0 (spatial size: 32x32)\n","        self.layer1 = self._make_layer(block, nstages[0], nstages[1], n)\n","        # Stage 1 (spatial size: 32x32)\n","        self.layer2 = self._make_layer(block, nstages[1], nstages[2], n, stride=2)\n","        # Stage 2 (spatial size: 16x16)\n","        self.layer3 = self._make_layer(block, nstages[2], nstages[3], n, stride=2)\n","        # Stage 3 (spatial size: 8x8)\n","        self.bn = nn.BatchNorm2d(nstages[3])\n","        self.relu = nn.ReLU(inplace=True)\n","        # classifier\n","        self.avgpool = nn.AvgPool2d(8)\n","        self.fc = nn.Linear(nstages[3], output_classes)\n","\n","        # weight initialization\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, inplanes, outplanes, nstage, stride=1):\n","        layers = []\n","        layers.append(block(inplanes, outplanes, stride))\n","        for i in range(1, nstage):\n","            layers.append(block(outplanes, outplanes, stride=1))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","\n","        x = self.relu(self.bn(x))\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XAnowr4f5yQ7"},"source":["def get_accuracy(model, dataloader):\n","  \"\"\"\n","    Function to compute accuracy given a model (of class nn) and a dataloader object\n","  \"\"\"\n","  \n","  model.eval()\n","  correct_predictions = 0\n","  with torch.no_grad():\n","    for images, labels in dataloader:\n","      imgs = images.to(device)\n","      lbls = labels.to(device)\n","      # images = images.view(images.shape[0], -1)\n","      output = model(imgs)\n","      _, predicted = torch.max(output.data, 1)\n","      correct_predictions += (predicted == lbls).sum().item()\n","  accuracy = (correct_predictions / len(dataloader.dataset)) * 100\n","  return(accuracy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTf6XRRc5y5Q"},"source":["def train_network(model, num_epochs, learning_rate, train_dataLoader, test_dataLoader, lr_update_rule):\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  train_accuracy_list, test_accuracy_list, train_loss_list = [], [], [] \n","  for epoch in range(num_epochs):\n","    running_loss = 0\n","    for images, labels in train_dataLoader:\n","      imgs = images.to(device)\n","      lbls = labels.to(device)\n","  \n","      # Training step\n","      optimizer.zero_grad()\n","      out = model(imgs)\n","      loss = criterion(out, lbls)\n","      \n","      # Backpropagate loss\n","      loss.backward()\n","      \n","      # Optimize weights\n","      optimizer.step()\n","      \n","      running_loss += loss.item()\n","    \n","    train_loss = running_loss/len(train_dataLoader)\n","    train_loss_list.append(train_loss)\n","    train_accuracy = get_accuracy(model, train_dataLoader)\n","    train_accuracy_list.append(train_accuracy)\n","    \n","    test_accuracy = get_accuracy(model, test_dataLoader)\n","    test_accuracy_list.append(test_accuracy)\n","    print(\"Epoch: {} \\t Training loss: {} \\t Training accuracy: {} \\t Test accuracy: {}\".format(epoch, train_loss, train_accuracy, test_accuracy))\n","    \n","  return model, train_accuracy_list, test_accuracy_list, train_loss_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWlRuYIj5005"},"source":["# Hyper-parameters\n","EPOCHS = 100\n","lr = 0.001\n","lr_update = {80:0.0001, 120:0.00001, 160:0.000001}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-rfRM9b5ltE"},"source":["resnet110B_ic2 = ResNet(Bottleneck_IC2, 110,10).to(device)\n","resnet110B_ic2, train_acc_110B_ic2, test_acc_110B_ic2, train_loss_110B_ic2 = train_network(model=resnet110B_ic2,\n","                                                                       num_epochs=EPOCHS,\n","                                                                       learning_rate=lr,\n","                                                                       train_dataLoader=train_dataLoader,\n","                                                                       test_dataLoader=test_dataLoader,\n","                                                                       lr_update_rule=lr_update)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lvp-DXrM5hZd"},"source":["import pandas as pd\n","metric_data_ic2 = pd.DataFrame({'Epoch': range(1,EPOCHS+1), 'Train_Acc': train_acc_110B_ic2, 'Test_Acc': test_acc_110B_ic2, 'Train_Loss': train_loss_110B_ic2})\n","metric_data_ic2.to_csv('ResNet110_B_ic2.csv', index=False)\n","torch.save(resnet110B_ic2, 'ResNet110_B_ic2.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gb89V6D55hDH"},"source":[""],"execution_count":null,"outputs":[]}]}